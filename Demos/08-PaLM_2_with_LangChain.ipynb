{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PaLM-2 with LangChain"
      ],
      "metadata": {
        "id": "GItUVBwhOPyh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRYSu48huSUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb46cb0-2ef5-4375-b6b9-20e4fbfb070c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.3/321.3 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m966.7/966.7 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install google-cloud-aiplatform google-api-python-client\n",
        "!pip -q install langchain google-search-results duckduckgo-search chromadb==0.3.29"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Must restart the Notebook runtime if in Colab etc"
      ],
      "metadata": {
        "id": "0NG-Bfb_zWy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setting up the Auth\n",
        "import os\n",
        "\n",
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"\"\n",
        "vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
      ],
      "metadata": {
        "id": "0gjPsekaxvSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "id": "J-KFB7J_u_3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9741563b-9f4e-4861-8cf1-e0f3524d0a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.235\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://www.github.com/hwchase17/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, dataclasses-json, langsmith, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google's PaLM-2 with LangChain\n"
      ],
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using the model in the Chat format"
      ],
      "metadata": {
        "id": "1hUi6GLZ6Sdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "from langchain.chat_models import ChatVertexAI\n",
        "from langchain.llms import VertexAI\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
      ],
      "metadata": {
        "id": "IfCt8bhHNu9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard LLM completion model\n",
        "llm = VertexAI(\n",
        "    model_name=\"text-bison@001\",\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Chat\n",
        "chat = ChatVertexAI(model='chat-bison',\n",
        "                    max_output_tokens=256,\n",
        "                    temperature=0.1,\n",
        "                    top_p=0.8,\n",
        "                    top_k=40,)\n"
      ],
      "metadata": {
        "id": "OYys7K9ltq6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    HumanMessage(\n",
        "        content=\"How do you compare to gpt-4?\"\n",
        "    )\n",
        "]\n",
        "chat(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaJlFRPPwGVf",
        "outputId": "2c4dd234-7de3-45db-ff33-1e827675d585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I am powered by PaLM 2, which stands for Pathways Language Model 2, a large language model from Google AI.', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using it as a basic ChatBot with memory and search"
      ],
      "metadata": {
        "id": "cWs-hmIb6g5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatVertexAI\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from langchain.agents import initialize_agent"
      ],
      "metadata": {
        "id": "v3RP-W3f6Rj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Chat\n",
        "chat_llm = ChatVertexAI(model='chat-bison',\n",
        "                    max_output_tokens=256,\n",
        "                    temperature=0.1,\n",
        "                    top_p=0.8,\n",
        "                    top_k=40,)\n"
      ],
      "metadata": {
        "id": "0dLpZPBv8UZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = DuckDuckGoSearchRun()\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Current Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to answer questions about current events or the current state of the world\"\n",
        "    ),\n",
        "]"
      ],
      "metadata": {
        "id": "qLsIq9n88eVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain = initialize_agent(tools,\n",
        "                               chat_llm,\n",
        "                               agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "                               verbose=True,\n",
        "                               memory=memory)"
      ],
      "metadata": {
        "id": "MHS_8G1kwPUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"hi, i am sam\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "Ti5GcCnH8COL",
        "outputId": "0db60818-c151-4b1c-a32a-861c3b285973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Hello Sam, how can I help you today?\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Sam, how can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"What is Elon Musk's x.ai?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "KRGRG_My9BNP",
        "outputId": "2e04243b-33d5-4e0e-f5da-ac791fa2572f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Current Search\",\n",
            "    \"action_input\": \"Elon Musk's x.ai\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mJuly 12 (Reuters) - Elon Musk, the billionaire entrepreneur, launched his long-teased artificial intelligence startup xAI on Wednesday, unveiling a team comprised of engineers from the same... Elon Musk has created a new company dedicated to artificial intelligence — and it's called X.AI, as first reported by The Wall Street Journal. The company, which a Nevada filing indicates... What to Know About Elon Musk's New AI Company, xAI Chief Executive Officer of SpaceX and Tesla and owner of Twitter, Elon Musk attends the Viva Technology conference on June 16, 2023 in Paris, France. Elon Musk, CEO of Tesla and SpaceX, and owner of Twitter, on Wednesday announced the debut of a new AI company, xAI, with the goal to \"understand the true nature of the universe.\" According... Elon Musk speaks at an event for SpaceX. (Jonathan Newton/The Washington Post) Elon Musk took to Twitter to talk about his new artificial intelligence company, xAI, outlining a goal of building a ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```json\n",
            "{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Elon Musk's x.ai is a new artificial intelligence company founded by Elon Musk. The company's goal is to understand the true nature of the universe.\"\n",
            "}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Elon Musk's x.ai is a new artificial intelligence company founded by Elon Musk. The company's goal is to understand the true nature of the universe.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"Who is on the team?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "NOvuSzT29QEy",
        "outputId": "b3ef53f7-de84-4f86-b618-a5d7c2fc0e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m{\n",
            "    \"action\": \"Current Search\",\n",
            "    \"action_input\": \"Elon Musk's x.ai team\"\n",
            "}\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mJuly 12 (Reuters) - Elon Musk, the billionaire entrepreneur, launched his long-teased artificial intelligence startup xAI on Wednesday, unveiling a team comprised of engineers from the... Elon Musk today (July 12) officially unveiled xAI, an artificial intelligence company he has been teasing for a little while, and its founding members, most of whom he poached from Google,... Elon Musk officially unveiled his new company xAI on Wednesday. Its mission is no less than to probe the \"true nature of the universe,\" according to its website. Musk was hiring for a large... xAI, Elon Musk's newly formed AI company, has revealed itself with a new website detailing its mission and team at https://x.ai/. Musk tweeted the company's intent is to \"understand... Dan Hendrycks, who will advise the xAI team, is currently director of the Center for AI Safety, and his work revolves around the risks of AI. Mr Musk's new company is separate from X Corp, but ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"The team at Elon Musk's x.ai is comprised of engineers from Google, OpenAI, and other top AI companies.\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The team at Elon Musk's x.ai is comprised of engineers from Google, OpenAI, and other top AI companies.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_chain.run(input=\"What do they plan to do?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "-rzhjdZ69iQt",
        "outputId": "249b3c71-aa2d-492f-f4fa-7e9c3afe9596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m{\n",
            "    \"action\": \"Current Search\",\n",
            "    \"action_input\": \"Elon Musk's x.ai\"\n",
            "}\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mJuly 12 (Reuters) - Elon Musk, the billionaire entrepreneur, launched his long-teased artificial intelligence startup xAI on Wednesday, unveiling a team comprised of engineers from the same... Elon Musk has created a new company dedicated to artificial intelligence — and it's called X.AI, as first reported by The Wall Street Journal. The company, which a Nevada filing indicates... What to Know About Elon Musk's New AI Company, xAI Chief Executive Officer of SpaceX and Tesla and owner of Twitter, Elon Musk attends the Viva Technology conference on June 16, 2023 in Paris, France. Elon Musk, CEO of Tesla and SpaceX, and owner of Twitter, on Wednesday announced the debut of a new AI company, xAI, with the goal to \"understand the true nature of the universe.\" According... Elon Musk took to Twitter to talk about his new artificial intelligence company, xAI, outlining a goal of building a super-intelligent AI that could help solve complex scientific and math...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m{\n",
            "    \"action\": \"Final Answer\",\n",
            "    \"action_input\": \"Elon Musk's x.ai plans to build a rival to ChatGPT.\"\n",
            "}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Elon Musk's x.ai plans to build a rival to ChatGPT.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using PaLM2 for Information Retrieval"
      ],
      "metadata": {
        "id": "qaFMtjBx-oqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# please put your own zip file of text files in here as for copyright reasons I can't share this one\n",
        "!wget -q -O elon_xai.zip\n",
        "!unzip -q elon_xai.zip -d xai_articles"
      ],
      "metadata": {
        "id": "DOhOL7FeHU5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "import time\n",
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "from langchain.embeddings import VertexAIEmbeddings"
      ],
      "metadata": {
        "id": "7Ib1wDM99qnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DirectoryLoader('./xai_articles/', glob=\"./*.txt\", loader_cls=TextLoader)\n",
        "\n",
        "documents = loader.load()\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCrGYCAoHPPR",
        "outputId": "9c382a29-98dd-4fe9-8a10-ed238e38d8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the text into\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "EHX-HFMtIqQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions for Embeddings API with rate limiting\n",
        "def rate_limit(max_per_minute):\n",
        "    period = 60 / max_per_minute\n",
        "    print(\"Waiting\")\n",
        "    while True:\n",
        "        before = time.time()\n",
        "        yield\n",
        "        after = time.time()\n",
        "        elapsed = after - before\n",
        "        sleep_time = max(0, period - elapsed)\n",
        "        if sleep_time > 0:\n",
        "            print(\".\", end=\"\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "\n",
        "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
        "    requests_per_minute: int\n",
        "    num_instances_per_batch: int\n",
        "\n",
        "    # Overriding embed_documents method\n",
        "    def embed_documents(self, texts: List[str]):\n",
        "        limiter = rate_limit(self.requests_per_minute)\n",
        "        results = []\n",
        "        docs = list(texts)\n",
        "\n",
        "        while docs:\n",
        "            # Working in batches because the API accepts maximum 5\n",
        "            # documents per request to get embeddings\n",
        "            head, docs = (\n",
        "                docs[: self.num_instances_per_batch],\n",
        "                docs[self.num_instances_per_batch :],\n",
        "            )\n",
        "            chunk = self.client.get_embeddings(head)\n",
        "            results.extend(chunk)\n",
        "            next(limiter)\n",
        "\n",
        "        return [r.values for r in results]"
      ],
      "metadata": {
        "id": "aA4DIsmnrvH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding\n",
        "EMBEDDING_QPM = 100\n",
        "EMBEDDING_NUM_BATCH = 5\n",
        "embeddings = CustomVertexAIEmbeddings(\n",
        "    requests_per_minute=EMBEDDING_QPM,\n",
        "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
        ")"
      ],
      "metadata": {
        "id": "FDQTadjtrP1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embedding = embeddings.embed_query(texts[0])\n",
        "print(f\"Your embedding is length {len(text_embedding)}\")\n",
        "print(f\"Here's a sample: {text_embedding[:5]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st5nTUs2riLy",
        "outputId": "39f3d0aa-01e3-4a5c-9434-531bf12d807c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your embedding is length 768\n",
            "Here's a sample: [0.008760017342865467, -0.03748807683587074, 0.01045919582247734, 0.03640196844935417, 0.046723198145627975]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# embedding_list = embeddings.embed_documents([text.page_content for text in texts])\n",
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PSeScoOI_zt",
        "outputId": "c4a7ca7c-5450-42bc-ae13-cd3090858738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomVertexAIEmbeddings(client=<vertexai.language_models._language_models._PreviewTextEmbeddingModel object at 0x7e6fe84ee560>, model_name='textembedding-gecko', temperature=0.0, max_output_tokens=128, top_p=0.95, top_k=40, stop=None, project=None, location='us-central1', credentials=None, request_parallelism=5, max_retries=6, requests_per_minute=100, num_instances_per_batch=5)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making a Chroma DB"
      ],
      "metadata": {
        "id": "F3nZSOvsJNDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed and store the texts\n",
        "# Supplying a persist_directory will store the embeddings on disk\n",
        "persist_directory = 'db'\n",
        "\n",
        "## Here is the nmew embeddings being used\n",
        "# embedding = embeddings\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                 embedding=embeddings,\n",
        "                                 persist_directory=persist_directory)\n"
      ],
      "metadata": {
        "id": "betEeqaRJFzj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63279672-40fc-46f9-becb-97affcef90c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting\n",
            "...."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making out retriever\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 7})"
      ],
      "metadata": {
        "id": "Nm4wHUPWJSW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the chain to answer questions\n",
        "# Chat\n",
        "llm = ChatVertexAI(model='chat-bison',\n",
        "                    max_output_tokens=256,\n",
        "                    temperature=0.1,\n",
        "                    top_p=0.8,\n",
        "                    top_k=40,)\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "VgmNTZ78KGy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did Elon announce?\"\n",
        "llm_response = qa_chain(query)\n",
        "llm_response[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "xAl7TdZpK9W2",
        "outputId": "e7ce8faf-449f-452c-ffb9-9f41cfb15beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Elon Musk announced the formation of xAI, a new artificial intelligence company. The goal of xAI is to understand the true nature of the universe.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who is on the team?\"\n",
        "llm_response = qa_chain(query)\n",
        "llm_response[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2UfCAaoDUWO8",
        "outputId": "4daa6ed4-5775-4911-ec0d-9776cf4d3280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The team is led by Elon Musk, CEO of Tesla and SpaceX. The team also includes Igor Babuschkin, Tony Wu, Christian Szegedy, Greg Yang, Dan Hendrycks, and others.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is he going to call his GPT model?\"\n",
        "llm_response = qa_chain(query)\n",
        "llm_response[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GgTLNhO9Vr59",
        "outputId": "6a8ff902-930f-4e50-e72a-842f05221597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Musk has not yet announced the name of his GPT model.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who is going to be ceo?\"\n",
        "llm_response = qa_chain(query)\n",
        "llm_response[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "APBkXDOgWG4k",
        "outputId": "0feb432b-06c5-4f86-d3c0-596abc948eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Elon Musk is the CEO of xAI.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Where is the company going to be based?\"\n",
        "llm_response = qa_chain(query)\n",
        "llm_response[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WAuv6QPtWVEQ",
        "outputId": "b08668f3-88cf-4983-cac6-149941612b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The company will be based in the Bay Area.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did Musk say about China and AI?\"\n",
        "llm_response = qa_chain(query)\n",
        "print(llm_response[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DshAIHV4Wkox",
        "outputId": "e0b2d54e-3d9a-41da-cd63-c510ebce5b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Musk said that China is interested in working in a cooperative international framework for AI regulation. He added that he had advocated for artificial intelligence regulations and oversight, including in his meetings in China.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "faRJrE6-WxCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PAL Chain with PaLM2"
      ],
      "metadata": {
        "id": "izhztr6atoJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import PALChain\n",
        "from langchain import OpenAI\n",
        "from langchain.chains.llm import LLMChain"
      ],
      "metadata": {
        "id": "-lzO5PfUpwfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard LLM completion model\n",
        "llm = VertexAI(\n",
        "    model_name=\"text-bison@001\",\n",
        "    max_output_tokens=256,\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "qI_368pwuNDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pal_chain = PALChain.from_math_prompt(llm, verbose=True)"
      ],
      "metadata": {
        "id": "odJ9mqfHHjMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"The cafeteria had 23 apples. \\\n",
        "If they used 20 for lunch and bought 6 more,\\\n",
        "how many apples do they have?\""
      ],
      "metadata": {
        "id": "RS0GyY0xIA-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pal_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "aBYAo9OgKf2W",
        "outputId": "68a0ca75-cfd7-47b1-b79f-dfcf213df1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new PALChain chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mdef solution():\n",
            "    \"\"\"The cafeteria had 23 apples. If they used 20 for lunch and bought 6 more,how many apples do they have?\"\"\"\n",
            "    apples_initial = 23\n",
            "    apples_used = 20\n",
            "    apples_bought = 6\n",
            "    apples_left = apples_initial - apples_used + apples_bought\n",
            "    result = apples_left\n",
            "    return result\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'9'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GSM8k"
      ],
      "metadata": {
        "id": "cs8b3-DW_7Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"John buys twice as many red ties as blue ties. The red ties cost 50% more than blue ties. \\\n",
        "He spent $200 on blue ties that cost $40 each. How much did he spend on ties?\""
      ],
      "metadata": {
        "id": "_a5XP5k_Anff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pal_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "WXiqz42uAy0G",
        "outputId": "0636e646-6550-448b-8388-4e6505b9745a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new PALChain chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mdef solution():\n",
            "    \"\"\"John buys twice as many red ties as blue ties. The red ties cost 50% more than blue ties. He spent $200 on blue ties that cost $40 each. How much did he spend on ties?\"\"\"\n",
            "    blue_ties = 200 / 40\n",
            "    red_ties = blue_ties * 2\n",
            "    red_ties_cost = red_ties * 40 * 1.5\n",
            "    total_cost = red_ties_cost + blue_ties * 40\n",
            "    result = total_cost\n",
            "    return result\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'800.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Maggie spent a quarter of her money, while Riza spent one-third of her money. \\\n",
        "They each had $60. How much money do the two of them have left?\""
      ],
      "metadata": {
        "id": "ViiU4blmK6mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pal_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "Emgw7lKaK_KQ",
        "outputId": "4636c128-ecf7-4687-93d0-74ce02bd4cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new PALChain chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mdef solution():\n",
            "    \"\"\"Maggie spent a quarter of her money, while Riza spent one-third of her money. They each had $60. How much money do the two of them have left?\"\"\"\n",
            "    maggie_money = 60\n",
            "    maggie_spent = maggie_money / 4\n",
            "    riza_money = 60\n",
            "    riza_spent = riza_money / 3\n",
            "    total_spent = maggie_spent + riza_spent\n",
            "    total_money_left = maggie_money - total_spent\n",
            "    result = total_money_left\n",
            "    return result\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'25.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JaVMaMWFxYrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Maggie and Riza each had $60. Maggie spent a quarter of her money, while Riza spent one-third of her money. \\\n",
        " How much money do the two of them have left?\""
      ],
      "metadata": {
        "id": "R4bY33xMxY7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pal_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "c7eba39c-325c-4d33-e2a3-fe8e909d5aba",
        "id": "Djc9rXENxY7s"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new PALChain chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mdef solution():\n",
            "    \"\"\"Maggie and Riza each had $60. Maggie spent a quarter of her money, while Riza spent one-third of her money.  How much money do the two of them have left?\"\"\"\n",
            "    maggie_money = 60\n",
            "    maggie_spent = maggie_money / 4\n",
            "    riza_money = 60\n",
            "    riza_spent = riza_money / 3\n",
            "    maggie_left = maggie_money - maggie_spent\n",
            "    riza_left = riza_money - riza_spent\n",
            "    total_left = maggie_left + riza_left\n",
            "    result = total_left\n",
            "    return result\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'85.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"In November, a toy was $1000. \\\n",
        "In December, the price increased by 80%. In January, the price decreased by 50%. \\\n",
        "What was the price of the toy after it was discounted in January?\""
      ],
      "metadata": {
        "id": "cYveyvWXLSzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pal_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ZIQ_6tsQLYgI",
        "outputId": "31cf0611-4add-4df2-fc98-669b30fda433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new PALChain chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mdef solution():\n",
            "    \"\"\"In November, a toy was $1000. In December, the price increased by 80%. In January, the price decreased by 50%. What was the price of the toy after it was discounted in January?\"\"\"\n",
            "    toy_price_initial = 1000\n",
            "    toy_price_december = toy_price_initial * 1.8\n",
            "    toy_price_january = toy_price_december * 0.5\n",
            "    result = toy_price_january\n",
            "    return result\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'900.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repeat words"
      ],
      "metadata": {
        "id": "HUOUl_HRL6Cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Repeat cheese seven times; every third say whiz\""
      ],
      "metadata": {
        "id": "_MJZozf6L9Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pal_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "SStFPIwaMCod",
        "outputId": "07911ebf-fb98-4045-d157-c028d945d570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new PALChain chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mdef solution():\n",
            "    \"\"\"Repeat cheese seven times; every third say whiz\"\"\"\n",
            "    for i in range(1, 8):\n",
            "        if i % 3 == 0:\n",
            "            print(\"cheese whiz\")\n",
            "        else:\n",
            "            print(\"cheese\")\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cheese\\ncheese\\ncheese whiz\\ncheese\\ncheese\\ncheese whiz\\ncheese\\nNone'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Say the letters of the alphabet in capital letters, but only the odd ones\"\n",
        "#A C E G I K M O Q S U W Y\""
      ],
      "metadata": {
        "id": "D7gfcvKcMErB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pal_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "ltzGYvqOME4d",
        "outputId": "747d1710-ac76-4071-cf0b-d5ba5d99b475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new PALChain chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mdef solution():\n",
            "    \"\"\"Say the letters of the alphabet in capital letters, but only the odd ones\"\"\"\n",
            "    alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
            "    odd_letters = alphabet[1::2]\n",
            "    result = odd_letters\n",
            "    return result\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BDFHJLNPRTVXZ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V0PJqvcCvUn8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}